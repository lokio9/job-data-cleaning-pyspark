# ------------------------------
# 1️⃣ Imports and Config
# ------------------------------
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, trim, lower, regexp_replace, split, concat_ws, lit, from_unixtime, year, explode_outer, avg, min, max, count, desc
from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, DateType
from functools import reduce

# ------------------------------
# Spark session
# ------------------------------
spark = SparkSession.builder.appName("JobDataCleaningPipeline").getOrCreate()

# ------------------------------
# Paths
# ------------------------------
INPUT_PATHS = [
    "/Volumes/workspace/bronze/raw/inputs/postings.csv",
    "/Volumes/workspace/bronze/new/job_descriptions_data/",
    "/Volumes/workspace/bronze/new/jobstreet/jobstreet_all_job_dataset.csv",
    "/Volumes/workspace/bronze/new/dfall_clean.csv"
]
OUTPUT_DIR = "/Volumes/workspace/bronze/new/to_merge/"
MERGED_OUTPUT = "/Volumes/workspace/bronze/new/to_merge/merged_location/"

# ------------------------------
# 2️⃣ Function: Read and clean each dataset
# ------------------------------
def clean_postings(df):
    df = df.withColumn("original_listed_time_casted", col("original_listed_time").cast("double"))
    df = df.withColumn("listed_time_readable", from_unixtime((col("original_listed_time_casted")/1000).cast("long")).cast("date"))
    df = df.withColumn("job_id", col("job_id").cast("long")) \
           .withColumn("normalized_salary", col("normalized_salary").cast("double"))
    selected_cols = ["job_id","company_name","title","description","pay_period","location","company_id",
                     "formatted_work_type","remote_allowed","skills_desc","currency","normalized_salary",
                     "zip_code","listed_time_readable"]
    return df.select(*[col(c) for c in selected_cols])

def clean_job_descriptions(df):
    # Salary cleaning
    df = df.withColumn("salary_range_clean", regexp_replace(col("Salary Range"), "[₹$ K]", ""))
    df = df.withColumn("min_salary", split(col("salary_range_clean"), "-").getItem(0).cast("int") * 1000)
    df = df.withColumn("max_salary", split(col("salary_range_clean"), "-").getItem(1).cast("int") * 1000)
    df = df.withColumn("median_salary", ((col("min_salary") + col("max_salary"))/2).cast("double"))

    # Rename & add columns
    df = df.withColumnRenamed("Job Id","job_id") \
           .withColumnRenamed("Company","company_name") \
           .withColumnRenamed("Job Title","title") \
           .withColumnRenamed("Job Description","description") \
           .withColumnRenamed("Work Type","formatted_work_type") \
           .withColumnRenamed("skills","skills_desc") \
           .withColumnRenamed("median_salary","normalized_salary") \
           .withColumnRenamed("Job Posting Date","listed_time_readable") \
           .withColumn("pay_period", lit("yearly")) \
           .withColumn("location", concat_ws(", ", col("Country"), col("location"))) \
           .withColumn("company_id", lit("1")) \
           .withColumn("remote_allowed", lit("-1")) \
           .withColumn("currency", lit("")) \
           .withColumn("zip_code", lit(""))

    selected_cols = ["job_id","company_name","title","description","pay_period","location","company_id",
                     "formatted_work_type","remote_allowed","skills_desc","currency","normalized_salary",
                     "zip_code","listed_time_readable"]
    return df.select(*[col(c) for c in selected_cols])

def clean_jobstreet(df):
    df = df.withColumn("pay_period", lit("yearly")) \
           .withColumn("company_id", lit("")) \
           .withColumn("remote_allowed", lit("-1")) \
           .withColumn("currency", lit("RM")) \
           .withColumn("zip_code", lit("")) \
           .withColumnRenamed("company","company_name") \
           .withColumnRenamed("job_title","title") \
           .withColumnRenamed("descriptions","description") \
           .withColumnRenamed("type","formatted_work_type") \
           .withColumnRenamed("subcategory","skills_desc") \
           .withColumn("listingDate", col("listingDate").cast("date")) \
           .withColumnRenamed("listingDate","listed_time_readable")

    # Salary parsing
    df = df.withColumn("salary_clean", regexp_replace(col("salary"), r"RM|,|\u00A0", ""))
    df = df.withColumn("min_salary_str", regexp_extract(col("salary_clean"), r"(\d+)", 1)) \
           .withColumn("max_salary_str", regexp_extract(col("salary_clean"), r"[–—-]\s*(\d+)", 1))
    df = df.withColumn("min_salary", col("min_salary_str").cast("int")) \
           .withColumn("max_salary", col("max_salary_str").cast("int"))
    df = df.withColumn("max_salary", coalesce(col("max_salary"), col("min_salary")))
    df = df.withColumn("med_salary", ((col("min_salary") + col("max_salary"))/2).cast("double"))
    df = df.drop("salary_clean","min_salary_str","max_salary_str","avg_salary") \
           .withColumnRenamed("med_salary","normalized_salary")

    selected_cols = ["job_id","company_name","title","description","pay_period","location","company_id",
                     "formatted_work_type","remote_allowed","skills_desc","currency","normalized_salary",
                     "zip_code","listed_time_readable"]
    return df.select(*[col(c) for c in selected_cols])

def clean_dfall(df):
    df = df.withColumnRenamed("id","job_id") \
           .withColumn("description", lit("NA")) \
           .withColumn("pay_period", lit("yearly")) \
           .withColumn("location", concat_ws(",", col("country"), col("location_display"))) \
           .withColumnRenamed("contract_time","formatted_work_type") \
           .withColumn("remote_allowed", lit("-1")) \
           .withColumn("skills_desc", lit("")) \
           .withColumn("currency", lit("")) \
           .withColumnRenamed("salary_mid","normalized_salary") \
           .withColumn("zip_code", lit("")) \
           .withColumnRenamed("created_date","listed_time_readable")

    selected_cols = ["job_id","company_name","title","description","pay_period","location","company_id",
                     "formatted_work_type","remote_allowed","skills_desc","currency","normalized_salary",
                     "zip_code","listed_time_readable"]
    return df.select(*[col(c) for c in selected_cols])


dfs = []

# Dataset 1
df0 = spark.read.option("header","true").option("inferSchema","true").option("multiline","true").csv(INPUT_PATHS[0])
dfs.append(clean_postings(df0))

# Dataset 2
df1 = spark.read.option("header","true").option("inferSchema","true").option("multiline","true").csv(INPUT_PATHS[1])
dfs.append(clean_job_descriptions(df1))

# Dataset 3
df3 = spark.read.option("header","true").option("inferSchema","true").option("multiline","true").csv(INPUT_PATHS[2])
dfs.append(clean_jobstreet(df3))

# Dataset 4
df4 = spark.read.option("header","true").option("inferSchema","true").option("multiline","true").csv(INPUT_PATHS[3])
dfs.append(clean_dfall(df4))

# Merge all
final_df = reduce(lambda a,b: a.unionByName(b), dfs)
final_df.coalesce(1).write.option("header","true").mode("overwrite").csv(MERGED_OUTPUT)
display(final_df)



# Bronze: drop rows without job_id
bronze_df = final_df.dropna(subset=["job_id"])

# Silver: explode skills
silver_df = bronze_df.withColumn("skill", explode_outer(split(regexp_replace(lower(col("skills_desc")), "[\\n;]", ","), ",| or ")))
silver_df = silver_df.dropna(subset=["company_name"])

# Gold: aggregations
gold_company = silver_df.groupBy("company_name").agg(count("job_id").alias("job_count"))
gold_year = silver_df.groupBy(year("listed_time_readable").alias("year")).agg(count("job_id").alias("jobs_per_year"))
gold_salary = silver_df.groupBy("company_name").agg(avg("normalized_salary").alias("avg_salary"),
                                                    min("normalized_salary").alias("min_salary"),
                                                    max("normalized_salary").alias("max_salary"))
gold_skills = silver_df.groupBy("skill").agg(count("*").alias("skill_count")).orderBy(desc("skill_count"))
gold_pay = silver_df.groupBy("pay_period").agg(count("*").alias("jobs_count"))




5️⃣ Optional: Dynamic country extraction

from geotext import GeoText
import country_converter as coco
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

def extract_country_smart(location):
    if not location:
        return "NA"
    loc = location.strip()
    places = GeoText(loc)
    if places.country_mentions:
        return list(places.country_mentions.keys())[0]
    elif places.cities:
        try:
            country = coco.convert(names=places.cities[0], to='name_short')
            if country != "not found": return country
        except: pass
    try:
        country = coco.convert(names=loc, to='name_short')
        if country != "not found": return country
    except: pass
    return "NA"

country_udf = udf(extract_country_smart, StringType())
bronze_df = bronze_df.withColumn("country", country_udf(col("location")))
display(bronze_df.select("location","country").distinct())


